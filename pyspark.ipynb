{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbVX0_sT_znx"
      },
      "source": [
        "## Source data & install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiMTznds-0ZO"
      },
      "source": [
        "# Install the dependencies\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\r\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\r\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcs5bNNZAKOq"
      },
      "source": [
        "# Set the environment variables for running PySpark in the collaboration environmentimport os\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Z7zq-PALTo"
      },
      "source": [
        "# Run the local session to test the installation\r\n",
        "import findspark\r\n",
        "findspark.init('spark-3.0.1-bin-hadoop3.2')\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oUUjv6oKTo8"
      },
      "source": [
        "# Create dataframe\r\n",
        "projects = spark.createDataFrame(\r\n",
        "                              [ (1, '2020-01-01', '2020-01-02'),\r\n",
        "                                (2, '2020-01-02', '2020-01-03'),\r\n",
        "                                (3, '2020-01-03', '2020-01-04'),\r\n",
        "                                (4, '2020-01-04', '2020-01-05'),\r\n",
        "                                (5, '2020-01-06', '2020-01-07'),\r\n",
        "                                (6, '2020-01-16', '2020-01-17'),\r\n",
        "                                (7, '2020-01-17', '2020-01-18'),\r\n",
        "                                (8, '2020-01-18', '2020-01-19'),\r\n",
        "                                (9, '2020-01-19', '2020-01-20'),\r\n",
        "                                (10, '2020-01-21', '2020-01-22'),\r\n",
        "                                (11, '2020-01-26', '2020-01-27'),\r\n",
        "                                (12, '2020-01-27', '2020-01-28'),\r\n",
        "                                (13, '2020-01-28', '2020-01-29'),\r\n",
        "                                (14, '2020-01-29', '2020-01-30'),],\r\n",
        "                              ['proj_id' , 'proj_start', 'proj_end'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vCD5559QlBc",
        "outputId": "0887d32b-24b5-4a66-cdf4-523fecf33fd4"
      },
      "source": [
        "projects.show()\r\n",
        "projects.printSchema()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+\n",
            "|proj_id|proj_start|  proj_end|\n",
            "+-------+----------+----------+\n",
            "|      1|2020-01-01|2020-01-02|\n",
            "|      2|2020-01-02|2020-01-03|\n",
            "|      3|2020-01-03|2020-01-04|\n",
            "|      4|2020-01-04|2020-01-05|\n",
            "|      5|2020-01-06|2020-01-07|\n",
            "|      6|2020-01-16|2020-01-17|\n",
            "|      7|2020-01-17|2020-01-18|\n",
            "|      8|2020-01-18|2020-01-19|\n",
            "|      9|2020-01-19|2020-01-20|\n",
            "|     10|2020-01-21|2020-01-22|\n",
            "|     11|2020-01-26|2020-01-27|\n",
            "|     12|2020-01-27|2020-01-28|\n",
            "|     13|2020-01-28|2020-01-29|\n",
            "|     14|2020-01-29|2020-01-30|\n",
            "+-------+----------+----------+\n",
            "\n",
            "root\n",
            " |-- proj_id: long (nullable = true)\n",
            " |-- proj_start: string (nullable = true)\n",
            " |-- proj_end: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPTHqFLKMG5s"
      },
      "source": [
        "# Change type from string to date\r\n",
        "from datetime import datetime\r\n",
        "from pyspark.sql.functions import col, udf\r\n",
        "from pyspark.sql.types import *\r\n",
        "udf_date = udf(lambda x:datetime.strptime(x, \"%Y-%m-%d\"),DateType())\r\n",
        "df = projects.withColumn('proj_start',udf_date(col('proj_start'))).withColumn('proj_end',udf_date(col('proj_end')))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UwfBq6kNRTV",
        "outputId": "727ff959-268c-4ce8-96d9-d7870be45ba8"
      },
      "source": [
        "df.show()\r\n",
        "df.printSchema()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+\n",
            "|proj_id|proj_start|  proj_end|\n",
            "+-------+----------+----------+\n",
            "|      1|2020-01-01|2020-01-02|\n",
            "|      2|2020-01-02|2020-01-03|\n",
            "|      3|2020-01-03|2020-01-04|\n",
            "|      4|2020-01-04|2020-01-05|\n",
            "|      5|2020-01-06|2020-01-07|\n",
            "|      6|2020-01-16|2020-01-17|\n",
            "|      7|2020-01-17|2020-01-18|\n",
            "|      8|2020-01-18|2020-01-19|\n",
            "|      9|2020-01-19|2020-01-20|\n",
            "|     10|2020-01-21|2020-01-22|\n",
            "|     11|2020-01-26|2020-01-27|\n",
            "|     12|2020-01-27|2020-01-28|\n",
            "|     13|2020-01-28|2020-01-29|\n",
            "|     14|2020-01-29|2020-01-30|\n",
            "+-------+----------+----------+\n",
            "\n",
            "root\n",
            " |-- proj_id: long (nullable = true)\n",
            " |-- proj_start: date (nullable = true)\n",
            " |-- proj_end: date (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOkQ_NnDOYTF"
      },
      "source": [
        "## Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6kEk1oYOy6p"
      },
      "source": [
        "df.createOrReplaceTempView(\"tbl\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBxPdEXxPApQ"
      },
      "source": [
        "# Query\r\n",
        "df_sql = spark.sql(\"\"\"\r\n",
        "                      select \r\n",
        "                            p3.proj_group, \r\n",
        "                            min(p3.proj_start) as date_start,\r\n",
        "                            max(p3.proj_end) as date_end,\r\n",
        "                            datediff(max(p3.proj_end), min(p3.proj_end))+1 as delta\r\n",
        "                      from\r\n",
        "                          (select \r\n",
        "                            p2.*,\r\n",
        "                            sum(p2.flag)over(order by p2.proj_id) as proj_group\r\n",
        "                        from \r\n",
        "                          (select \r\n",
        "                                p.proj_id , \r\n",
        "                                p.proj_start, \r\n",
        "                                p.proj_end, \r\n",
        "                                case \r\n",
        "                                when lag(p.proj_end)over(order by p.proj_id) = p.proj_start then 0 else 1 \r\n",
        "                                end as flag\r\n",
        "                          from tbl as p) as p2) as p3\r\n",
        "                      group by p3.proj_group\r\n",
        "                    \"\"\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b79g7vfGPvKo",
        "outputId": "663715dd-2b9e-4192-a952-613367e1e2d7"
      },
      "source": [
        "df_sql.show()\r\n",
        "df_sql.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+-----+\n",
            "|proj_group|date_start|  date_end|delta|\n",
            "+----------+----------+----------+-----+\n",
            "|         1|2020-01-01|2020-01-05|    4|\n",
            "|         2|2020-01-06|2020-01-07|    1|\n",
            "|         3|2020-01-16|2020-01-20|    4|\n",
            "|         4|2020-01-21|2020-01-22|    1|\n",
            "|         5|2020-01-26|2020-01-30|    4|\n",
            "+----------+----------+----------+-----+\n",
            "\n",
            "root\n",
            " |-- proj_group: long (nullable = true)\n",
            " |-- date_start: date (nullable = true)\n",
            " |-- date_end: date (nullable = true)\n",
            " |-- delta: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b5Wg7CMxfjT"
      },
      "source": [
        "## Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r0zRKhPxlBE",
        "outputId": "8c454a65-1ce0-417a-e8b8-f64a6ad7a558"
      },
      "source": [
        "from pyspark.sql.functions import lag\r\n",
        "from pyspark.sql import functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "# Equivalent of Pandas.dataframe.shift() method\r\n",
        "w = Window().partitionBy().orderBy(col(\"proj_id\"))\r\n",
        "df_dataframe = df.withColumn('lag', F.lag(\"proj_end\").over(w))\r\n",
        "df_dataframe.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+----------+\n",
            "|proj_id|proj_start|  proj_end|       lag|\n",
            "+-------+----------+----------+----------+\n",
            "|      1|2020-01-01|2020-01-02|      null|\n",
            "|      2|2020-01-02|2020-01-03|2020-01-02|\n",
            "|      3|2020-01-03|2020-01-04|2020-01-03|\n",
            "|      4|2020-01-04|2020-01-05|2020-01-04|\n",
            "|      5|2020-01-06|2020-01-07|2020-01-05|\n",
            "|      6|2020-01-16|2020-01-17|2020-01-07|\n",
            "|      7|2020-01-17|2020-01-18|2020-01-17|\n",
            "|      8|2020-01-18|2020-01-19|2020-01-18|\n",
            "|      9|2020-01-19|2020-01-20|2020-01-19|\n",
            "|     10|2020-01-21|2020-01-22|2020-01-20|\n",
            "|     11|2020-01-26|2020-01-27|2020-01-22|\n",
            "|     12|2020-01-27|2020-01-28|2020-01-27|\n",
            "|     13|2020-01-28|2020-01-29|2020-01-28|\n",
            "|     14|2020-01-29|2020-01-30|2020-01-29|\n",
            "+-------+----------+----------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEvXtjAU5Wp3",
        "outputId": "13cf6eb3-8c3a-456d-92c9-9c0cf498ca89"
      },
      "source": [
        "# Equivalent of SQL- CASE WHEN...THEN...ELSE... END\r\n",
        "df_dataframe = df_dataframe.withColumn('flag',F.when(df_dataframe[\"proj_start\"] == df_dataframe[\"lag\"],0).otherwise(1))\r\n",
        "df_dataframe.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+----------+----+\n",
            "|proj_id|proj_start|  proj_end|       lag|flag|\n",
            "+-------+----------+----------+----------+----+\n",
            "|      1|2020-01-01|2020-01-02|      null|   1|\n",
            "|      2|2020-01-02|2020-01-03|2020-01-02|   0|\n",
            "|      3|2020-01-03|2020-01-04|2020-01-03|   0|\n",
            "|      4|2020-01-04|2020-01-05|2020-01-04|   0|\n",
            "|      5|2020-01-06|2020-01-07|2020-01-05|   1|\n",
            "|      6|2020-01-16|2020-01-17|2020-01-07|   1|\n",
            "|      7|2020-01-17|2020-01-18|2020-01-17|   0|\n",
            "|      8|2020-01-18|2020-01-19|2020-01-18|   0|\n",
            "|      9|2020-01-19|2020-01-20|2020-01-19|   0|\n",
            "|     10|2020-01-21|2020-01-22|2020-01-20|   1|\n",
            "|     11|2020-01-26|2020-01-27|2020-01-22|   1|\n",
            "|     12|2020-01-27|2020-01-28|2020-01-27|   0|\n",
            "|     13|2020-01-28|2020-01-29|2020-01-28|   0|\n",
            "|     14|2020-01-29|2020-01-30|2020-01-29|   0|\n",
            "+-------+----------+----------+----------+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAS5NYvg7xiI",
        "outputId": "6690a98b-1142-42d1-bff5-54e8df16959c"
      },
      "source": [
        "# Cumsum by column flag\r\n",
        "w = Window().partitionBy().orderBy(col(\"proj_id\"))\r\n",
        "df_dataframe = df_dataframe.withColumn(\"proj_group\", F.sum(\"flag\").over(w))\r\n",
        "df_dataframe.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+----------+----------+----+----------+\n",
            "|proj_id|proj_start|  proj_end|       lag|flag|proj_group|\n",
            "+-------+----------+----------+----------+----+----------+\n",
            "|      1|2020-01-01|2020-01-02|      null|   1|         1|\n",
            "|      2|2020-01-02|2020-01-03|2020-01-02|   0|         1|\n",
            "|      3|2020-01-03|2020-01-04|2020-01-03|   0|         1|\n",
            "|      4|2020-01-04|2020-01-05|2020-01-04|   0|         1|\n",
            "|      5|2020-01-06|2020-01-07|2020-01-05|   1|         2|\n",
            "|      6|2020-01-16|2020-01-17|2020-01-07|   1|         3|\n",
            "|      7|2020-01-17|2020-01-18|2020-01-17|   0|         3|\n",
            "|      8|2020-01-18|2020-01-19|2020-01-18|   0|         3|\n",
            "|      9|2020-01-19|2020-01-20|2020-01-19|   0|         3|\n",
            "|     10|2020-01-21|2020-01-22|2020-01-20|   1|         4|\n",
            "|     11|2020-01-26|2020-01-27|2020-01-22|   1|         5|\n",
            "|     12|2020-01-27|2020-01-28|2020-01-27|   0|         5|\n",
            "|     13|2020-01-28|2020-01-29|2020-01-28|   0|         5|\n",
            "|     14|2020-01-29|2020-01-30|2020-01-29|   0|         5|\n",
            "+-------+----------+----------+----------+----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuP-hHeW9TQ5",
        "outputId": "4d8d072d-1908-43d6-e584-65e95b521588"
      },
      "source": [
        "# Equivalent of SQL - GROUP BY\r\n",
        "from pyspark.sql.functions import  min, max\r\n",
        "df_group = df_dataframe.groupBy(\"proj_group\").agg(min(\"proj_start\").alias(\"date_start\"), \\\r\n",
        "                                                  max(\"proj_end\").alias(\"date_end\"))\r\n",
        "df_group = df_group.withColumn(\"delta\", F.datediff(df_group.date_end,df_group.date_start))\r\n",
        "df_group.show()\r\n",
        "df_group.printSchema()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+-----+\n",
            "|proj_group|date_start|  date_end|delta|\n",
            "+----------+----------+----------+-----+\n",
            "|         1|2020-01-01|2020-01-05|    4|\n",
            "|         2|2020-01-06|2020-01-07|    1|\n",
            "|         3|2020-01-16|2020-01-20|    4|\n",
            "|         4|2020-01-21|2020-01-22|    1|\n",
            "|         5|2020-01-26|2020-01-30|    4|\n",
            "+----------+----------+----------+-----+\n",
            "\n",
            "root\n",
            " |-- proj_group: long (nullable = true)\n",
            " |-- date_start: date (nullable = true)\n",
            " |-- date_end: date (nullable = true)\n",
            " |-- delta: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}